{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parsl\n",
    "import os\n",
    "from parsl.app.app import python_app, bash_app\n",
    "from parsl.configs.local_threads import config\n",
    "\n",
    "from parsl.providers import LocalProvider,CondorProvider\n",
    "from parsl.channels import LocalChannel,SSHChannel\n",
    "from parsl.config import Config\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "\n",
    "from parsl.addresses import address_by_hostname\n",
    "\n",
    "x509_proxy = 'x509up_u%s'%(os.getuid())\n",
    "year = '2018'\n",
    "\n",
    "wrk_init = '''\n",
    "source /cvmfs/sft.cern.ch/lcg/views/LCG_95apython3/x86_64-centos7-gcc7-opt/setup.sh\n",
    "export PATH=`pwd`/.local/bin:$PATH\n",
    "export PYTHONPATH=`pwd`/.local/lib/python3.6/site-packages:$PYTHONPATH\n",
    "\n",
    "export X509_USER_PROXY=`pwd`/%s\n",
    "mkdir -p decaf_parsl\n",
    "'''%(x509_proxy)\n",
    "\n",
    "twoGB = 2048\n",
    "nproc = 8\n",
    "\n",
    "condor_cfg = '''\n",
    "transfer_output_files = decaf_parsl\n",
    "RequestMemory = %d\n",
    "RequestCpus = %d\n",
    "''' % (twoGB*nproc, nproc)\n",
    "\n",
    "xfer_files = ['%s/.local' % (os.environ['HOME'], ), '%s/%s' % (os.environ['HOME'], x509_proxy, )]\n",
    "\n",
    "condor_htex = Config(\n",
    "    executors=[\n",
    "        HighThroughputExecutor(\n",
    "            label=\"decaf_parsl\",\n",
    "            address=address_by_hostname(),\n",
    "            prefetch_capacity=0,\n",
    "            cores_per_worker=1,\n",
    "            max_workers=nproc,\n",
    "            worker_logdir_root='./',\n",
    "            provider=CondorProvider(\n",
    "                channel=LocalChannel(),\n",
    "                init_blocks=16,\n",
    "                max_blocks=200,\n",
    "                nodes_per_block=1,\n",
    "                worker_init = wrk_init,                \n",
    "                transfer_input_files=xfer_files,\n",
    "                scheduler_options=condor_cfg\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    "    strategy=None,\n",
    ")\n",
    "\n",
    "#parsl.set_stream_logger() # <-- log everything to stdout\n",
    "\n",
    "dfk = parsl.load(condor_htex)\n",
    "\n",
    "chunksize=500000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lumis = {}\n",
    "#Values from https://twiki.cern.ch/twiki/bin/viewauth/CMS/PdmVAnalysisSummaryTable\n",
    "lumis['2016']=35.92\n",
    "lumis['2017']=41.53\n",
    "lumis['2018']=59.97\n",
    "lumi = 1000.*float(lumis[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "    \"iszeroL\":('ZJets','WJets','DY','TT','ST','WW','WZ','ZZ','QCD','HToBB','MET'),\n",
    "    \"isoneM\":('WJets','DY','TT','ST','WW','WZ','ZZ','QCD','HToBB','MET'),\n",
    "    \"isoneE\":('WJets','DY','TT','ST','WW','WZ','ZZ','QCD','HToBB','SingleElectron','EGamma'),\n",
    "    \"istwoM\":('WJets','DY','TT','ST','WW','WZ','ZZ','HToBB','MET'),\n",
    "    \"istwoE\":('WJets','DY','TT','ST','WW','WZ','ZZ','HToBB','SingleElectron','EGamma'),\n",
    "    \"isoneA\":('GJets','QCD','SinglePhoton','EGamma')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../harvester/beans/\"+year+\".json\") as fin:\n",
    "    samplefiles = json.load(fin)\n",
    "xsec = {k: v['xs'] for k,v in samplefiles.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'analysis.darkhiggs.AnalysisProcessor'>\n"
     ]
    }
   ],
   "source": [
    "#get the analysis worker from the cloudpickle file\n",
    "import cloudpickle as cpkl\n",
    "import lz4.frame as lz4f\n",
    "\n",
    "processor_pkl = 'AnalysisProcessor.cpkl.lz4'\n",
    "AnalysisProcessor = None\n",
    "with lz4f.open(processor_pkl, mode=\"rb\") as fin:\n",
    "    AnalysisProcessor = cpkl.load(fin)\n",
    "print(AnalysisProcessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsl version: 0.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|█████████▉| 5117/5125 [14:50<00:00,  9.53files/s]  "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from coffea import hist, processor\n",
    "from coffea.processor import run_parsl_job\n",
    "from coffea.processor.parsl.parsl_executor import parsl_executor\n",
    "import gzip\n",
    "import pickle\n",
    "import cloudpickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "filelist = {}\n",
    "for dt, info in samplefiles.items():\n",
    "    #if your_wanted_dataset not in dataset: continue\n",
    "    dataset = dt.strip().split(\"____\")[0]\n",
    "    if not dataset in filelist: filelist[dataset] = []\n",
    "    fileslice = slice(None)\n",
    "    for file in info['files'][fileslice]:\n",
    "        filelist[dataset].append(file)\n",
    "\n",
    "        \n",
    "selections = {}\n",
    "for dataset in filelist:\n",
    "    if not dataset in selections: selections[dataset] = []\n",
    "    for selection,v in samples.items():\n",
    "        for i in range (0,len(v)):\n",
    "            if v[i] not in dataset: continue\n",
    "    fileset = {}\n",
    "    fileset[dataset] = filelist[dataset]\n",
    "    processor_instance=AnalysisProcessor(selected_regions=selections[dataset], year=year, xsec=xsec, lumi=lumi)\n",
    "    tstart = time.time()\n",
    "    output = run_parsl_job(fileset,\n",
    "                           treename='Events',\n",
    "                           processor_instance=processor_instance,\n",
    "                           executor=parsl_executor,\n",
    "                           executor_args={'config':None},\n",
    "                           data_flow=dfk,\n",
    "                           chunksize=500000,\n",
    "                          )\n",
    "\n",
    "    # Pickle is not very fast or memory efficient, will be replaced by something better soon\n",
    "    with lz4f.open(\"pods/\"+year+\"/\"+dataset+\".pkl.gz\", mode=\"wb\", compression_level=5) as fout:\n",
    "        cloudpickle.dump(output, fout)\n",
    "        \n",
    "    dt = time.time() - tstart\n",
    "    nbins = sum(sum(arr.size for arr in h._sumw.values()) for h in output.values() if isinstance(h, hist.Hist))\n",
    "    nfilled = sum(sum(np.sum(arr > 0) for arr in h._sumw.values()) for h in output.values() if isinstance(h, hist.Hist))\n",
    "    print(\"Filled %.1fM bins\" % (nbins/1e6, ))\n",
    "    print(\"Nonzero bins: %.1f%%\" % (100*nfilled/nbins, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsl.dfk().cleanup()\n",
    "parsl.clear()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
