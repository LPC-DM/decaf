{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parsl\n",
    "import os\n",
    "from parsl.app.app import python_app, bash_app\n",
    "from parsl.configs.local_threads import config\n",
    "\n",
    "from parsl.providers import LocalProvider,CondorProvider\n",
    "from parsl.channels import LocalChannel,SSHChannel\n",
    "from parsl.config import Config\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "\n",
    "from parsl.addresses import address_by_hostname\n",
    "\n",
    "x509_proxy = 'x509up_u%s'%(os.getuid())\n",
    "year = '2018'\n",
    "\n",
    "wrk_init = '''\n",
    "source /cvmfs/sft.cern.ch/lcg/views/LCG_95apython3/x86_64-centos7-gcc7-opt/setup.sh\n",
    "export PATH=`pwd`/.local/bin:$PATH\n",
    "export PYTHONPATH=`pwd`/.local/lib/python3.6/site-packages:$PYTHONPATH\n",
    "\n",
    "export X509_USER_PROXY=`pwd`/%s\n",
    "mkdir -p decaf_parsl\n",
    "'''%(x509_proxy, year)\n",
    "\n",
    "twoGB = 2048\n",
    "nproc = 8\n",
    "\n",
    "condor_cfg = '''\n",
    "transfer_output_files = decaf_parsl\n",
    "RequestMemory = %d\n",
    "RequestCpus = %d\n",
    "''' % (twoGB*nproc, nproc)\n",
    "\n",
    "xfer_files = ['%s/.local' % (os.environ['HOME'], ), '%s/%s' % (os.environ['HOME'], x509_proxy, )]\n",
    "\n",
    "condor_htex = Config(\n",
    "    executors=[\n",
    "        HighThroughputExecutor(\n",
    "            label=\"decaf_parsl\",\n",
    "            address=address_by_hostname(),\n",
    "            prefetch_capacity=0,\n",
    "            cores_per_worker=1,\n",
    "            max_workers=nproc,\n",
    "            worker_logdir_root='./',\n",
    "            provider=CondorProvider(\n",
    "                channel=LocalChannel(),\n",
    "                init_blocks=16,\n",
    "                max_blocks=200,\n",
    "                nodes_per_block=1,\n",
    "                worker_init = wrk_init,                \n",
    "                transfer_input_files=xfer_files,\n",
    "                scheduler_options=condor_cfg\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    "    strategy=None,\n",
    ")\n",
    "\n",
    "#parsl.set_stream_logger() # <-- log everything to stdout\n",
    "\n",
    "dfk = parsl.load(condor_htex)\n",
    "\n",
    "chunksize=500000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lumis = {}\n",
    "#Values from https://twiki.cern.ch/twiki/bin/viewauth/CMS/PdmVAnalysisSummaryTable\n",
    "lumis['2016']=35.92\n",
    "lumis['2017']=41.53\n",
    "lumis['2018']=59.97\n",
    "lumi = 1000.*float(lumis[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../harvester/beans/\"+year+\".json\") as fin:\n",
    "    samplefiles = json.load(fin)\n",
    "xsec = {k: v['xs'] for k,v in samplefiles.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from coffea import hist, processor\n",
    "from analysis.darkhiggs import AnalysisProcessor,samples\n",
    "import gzip\n",
    "import pickle\n",
    "import cloudpickle\n",
    "import numpy as np\n",
    "\n",
    "filelist = {}\n",
    "selections = []\n",
    "fileslice = slice(None)\n",
    "\n",
    "for dataset, info in samplefiles.items():\n",
    "    #if your_wanted_dataset not in dataset: continue\n",
    "    files = []\n",
    "    for file in info['files'][fileslice]:\n",
    "        files.append(file)\n",
    "    filelist[dataset] = files\n",
    "    for selection,v in samples.items():\n",
    "        for i in range (0,len(v)):\n",
    "            if v[i] not in dataset: continue\n",
    "            selections.append(selection)\n",
    "\n",
    "    processor_instance=AnalysisProcessor(selected_regions=selections, year=year, xsec=xsec, lumi=lumi)\n",
    "    tstart = time.time()\n",
    "    output = processor.run_parsl_job(filelist,\n",
    "                                      treename='Events',\n",
    "                                      processor_instance=processor_instance,\n",
    "                                      executor=processor.parsl_executor,\n",
    "                                      executor_args={'config':None},\n",
    "                                      data_flow=dfk\n",
    "                                      chunksize=500000,\n",
    "                                      )\n",
    "    nbins = sum(sum(arr.size for arr in h._sumw.values()) for h in output.values() if isinstance(h, hist.Hist))\n",
    "    nfilled = sum(sum(np.sum(arr > 0) for arr in h._sumw.values()) for h in output.values() if isinstance(h, hist.Hist))\n",
    "    print(\"Filled %.1fM bins\" % (nbins/1e6, ))\n",
    "    print(\"Nonzero bins: %.1f%%\" % (100*nfilled/nbins, ))\n",
    "\n",
    "    # Pickle is not very fast or memory efficient, will be replaced by something better soon\n",
    "    #    with lz4f.open(\"pods/\"+options.year+\"/\"+dataset+\".pkl.gz\", mode=\"xb\", compression_level=5) as fout:\n",
    "    with gzip.open(\"pods/\"+year+\"/\"+dataset+\".pkl.gz\", \"wb\") as fout:\n",
    "        cloudpickle.dump(output, fout)\n",
    "        \n",
    "    dt = time.time() - tstart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsl.dfk().cleanup()\n",
    "parsl.clear()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
